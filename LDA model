LDA(Latent Dirichlet Allocation)是一种文档主题生成模型，也成为三层贝叶斯概率模型，包含词、文档、主题三层结构。
所谓生成模型，就是说，我们认为一篇文档的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。
文档到主题服从多项式分布，主题到词服从多项式分布。
LDA的目的就是识别主题，即把文档-词汇矩阵变成文档-主题矩阵和主题-词汇矩阵。

什么是多项式分布？
举例说明：掷骰子问题。


LDA生成过程：
   1.对每一篇文档，从主题分布中抽取一个主题；
   2.从上述被抽到的主题所对应的单词分布中抽取一个单词；
   3.重复上述过程直至遍历文档中每一个单词。
语料库中每一篇文档与T个主题的一个多项式分布相对应，记为θ（通过反复试验等方法事先给定）；每个主题又与词汇表中的V个
单词的一个多项式分布相对应，将这个多项式分布记为φ。

文档通过聚类得到主题（topic）
文档d对应第i个主题的概率=文档d中对应第i个topic的词的数目/文档d中词的总数
主题t生成某个单词i的概率=主题t中单词i的个数/主题t中的单词总数

LDA的核心公式如下：
p(w|d)=p(w|t)*p(t|d)
直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。
其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。
实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，根据这些结果来更新这个词应该对应的topic。
然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt。

学习过程：
先随机为θd和φt赋值。然后不断重复上述过程，最终收敛的结果就是LDA的输出。
1.针对一个特定的文档ds中的第i单词wi，如果令该单词对应的topic为tj，可以把上述公式改写为：
pj(wi|ds)=p(wi|tj)*p(tj|ds)
2.现在我们可以枚举T中的topic，得到所有的pj(wi|ds)，其中j取值1~k。
然后可以根据这些概率值结果为ds中的第i个单词wi选择一个topic。
最简单的想法是取令pj(wi|ds)最大的tj（注意，这个式子里只有j是变量），即argmax[j]pj(wi|ds)
3.然后，如果ds中的第i个单词wi在这里选择了一个与原先不同的topic，就会对θd和φt有影响了。
它们的影响又会反过来影响对上面提到的p(w|d)的计算。
对D中所有的d中的所有w进行一次p(w|d)的计算并重新选择topic看作一次迭代。
这样进行n次循环迭代之后，就会收敛到LDA所需要的结果了。

实例：
计算文档-词汇矩阵；
计算主题-词汇矩阵；（随机过程）
   得到每个词在某个主题下的概率（每个主题下出现的该词的个数/某个主题包含的所有词的个数）
   得到每个主题属于每个词的概率分布（每个单词在某主题下的个数/单词总个数）
计算文档-主题矩阵；（统计每个词代表的主题在每一个文档中出现的次数）
   
